<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>deeprecsys.neural_networks.dueling API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deeprecsys.neural_networks.dueling</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Any, List, Optional, Tuple

from numpy import array, ravel
from torch import BoolTensor, FloatTensor, LongTensor, Tensor, gather
from torch.nn import Linear, Module, ReLU, Sequential, functional
from torch.optim import Adam

from deeprecsys.neural_networks.base_network import BaseNetwork
from deeprecsys.neural_networks.noisy_layer import NoisyLayer
from deeprecsys.rl.experience_replay.priority_replay_buffer import (
    PrioritizedExperienceReplayBuffer,
)
from deeprecsys.rl.learning_statistics import LearningStatistics


class DuelingDDQN(BaseNetwork):
    &#34;&#34;&#34;Dueling DQN with Double DQN and Noisy Networks&#34;&#34;&#34;

    def __init__(
        self,
        n_input: int,
        n_output: int,
        learning_rate: float,
        hidden_layers: List[int] = None,
        noise_sigma: float = 0.17,
        discount_factor: float = 0.99,
        statistics: Optional[LearningStatistics] = None,
    ):
        &#34;&#34;&#34;Initialize the network with the provided parameters&#34;&#34;&#34;
        super().__init__()
        if not hidden_layers:
            hidden_layers = [256, 256, 64, 64]
        self.discount_factor = discount_factor
        self._build_network(n_input, n_output, noise_sigma, hidden_layers=hidden_layers)
        self.optimizer = Adam(self.parameters(), lr=learning_rate)
        self.statistics = statistics

    def _build_network(self, n_input: int, n_output: int, noise_sigma: float, hidden_layers: List[int]) -&gt; None:
        &#34;&#34;&#34;Build the dueling network with noisy layers, the value
        subnet and the advantage subnet. TODO: add `.to_device()` to Modules
        &#34;&#34;&#34;
        if len(hidden_layers) != 4:
            raise ValueError(&#34;Unexpected amount of layers&#34;)
        fc_1, fc_2, value_size, advantage_size = hidden_layers
        self.fully_connected_1 = Linear(n_input, fc_1, bias=True)
        self.fully_connected_2 = NoisyLayer(fc_1, fc_2, bias=True, sigma=noise_sigma)
        self.value_subnet = Sequential(
            NoisyLayer(fc_2, value_size, bias=True, sigma=noise_sigma),
            ReLU(),
            Linear(value_size, 1, bias=True),
        )
        self.advantage_subnet = Sequential(
            NoisyLayer(fc_2, advantage_size, bias=True, sigma=noise_sigma),
            ReLU(),
            Linear(advantage_size, n_output, bias=True),
        )

    def forward(self, state: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;Calculate the forward between the layers&#34;&#34;&#34;
        layer_1_out = functional.relu(self.fully_connected_1(state))
        layer_2_out = functional.relu(self.fully_connected_2(layer_1_out))
        value_of_state = self.value_subnet(layer_2_out)
        advantage_of_state = self.advantage_subnet(layer_2_out)
        # This is the Dueling DQN part
        # Combines V and A to get Q: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a&#39;))
        if len(state.shape) == 2:
            q_values = value_of_state + (advantage_of_state - advantage_of_state.mean(dim=1, keepdim=True))
        else:
            q_values = value_of_state + (advantage_of_state - advantage_of_state.mean())
        return q_values

    def get_q_values(self, state: Any) -&gt; Tensor:
        &#34;&#34;&#34;Run the state through the network and return the Q-value for each action.&#34;&#34;&#34;
        if type(state) is tuple:
            state = array([ravel(s) for s in state])
        state_tensor = FloatTensor(state).to(device=self.device)
        return self.forward(state_tensor)

    def top_k_actions_for_state(self, state: Any, k: int = 1) -&gt; List[int]:
        &#34;&#34;&#34;Get the top K actions ranked by their estimated Q-value.&#34;&#34;&#34;
        q_values = self.get_q_values(state)
        _, top_indices = q_values.topk(k=k)
        return [int(v) for v in top_indices.detach().numpy()]  # TODO: cpu() ?

    def learn_with(self, buffer: PrioritizedExperienceReplayBuffer, target_network: Module) -&gt; None:
        &#34;&#34;&#34;Train the target network using the replay buffer.&#34;&#34;&#34;
        experiences = buffer.sample_batch()
        self.optimizer.zero_grad()
        td_error, weights = self._calculate_td_error_and_weigths(experiences, target_network)
        loss = (td_error.pow(2) * weights).mean().to(self.device)
        loss.backward()
        self.optimizer.step()
        # store loss in statistics
        if self.statistics:
            if self.device == &#34;cuda&#34;:
                self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().cpu().numpy()))
            else:
                self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().numpy()))
        # update buffer priorities
        errors_from_batch = td_error.detach().cpu().numpy().flatten()
        buffer.update_priorities(experiences, errors_from_batch)

    def _calculate_td_error_and_weigths(
        self, experiences: List[Tuple], target_network: Module
    ) -&gt; Tuple[Tensor, Tensor]:
        states, actions, rewards, dones, next_states, weights, samples = [i for i in experiences]
        # convert to tensors
        state_tensors = FloatTensor(states).to(device=self.device)
        next_state_tensors = FloatTensor(next_states).to(device=self.device)
        reward_tensors = FloatTensor(rewards).to(device=self.device).reshape(-1, 1)
        action_tensors = LongTensor(array(actions)).reshape(-1, 1).to(device=self.device)
        done_tensors = BoolTensor(dones).to(device=self.device)
        weight_tensors = FloatTensor(weights).to(device=self.device)
        # the following logic is the DDQN update
        # Then we get the predicted actions for the states that came next
        # (using the main network)
        actions_for_next_states = [self.top_k_actions_for_state(s)[0] for s in next_state_tensors]
        actions_for_next_states_tensor = LongTensor(actions_for_next_states).reshape(-1, 1).to(device=self.device)
        # Then we use them to get the estimated Q Values for these next states/actions,
        # according to the target network. Remember that the target network is a copy
        # of this one taken some steps ago
        next_q_values = target_network.forward(next_state_tensors)
        # now we get the q values for the actions that were predicted for the next state
        # we call detach() so no gradient will be backpropagated along this variable
        next_q_values_for_actions = gather(next_q_values, 1, actions_for_next_states_tensor).detach()
        # zero value for done timesteps
        next_q_values_for_actions[done_tensors] = 0
        # bellman equation
        expected_q_values = self.discount_factor * next_q_values_for_actions + reward_tensors
        # Then get the Q-Values of the main network for the selected actions
        q_values = gather(self.forward(state_tensors), 1, action_tensors)
        # And compare them (this is the time-difference or TD error)
        td_error = q_values - expected_q_values
        return td_error, weight_tensors.reshape(-1, 1)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deeprecsys.neural_networks.dueling.DuelingDDQN"><code class="flex name class">
<span>class <span class="ident">DuelingDDQN</span></span>
<span>(</span><span>n_input: int, n_output: int, learning_rate: float, hidden_layers: List[int] = None, noise_sigma: float = 0.17, discount_factor: float = 0.99, statistics: Optional[<a title="deeprecsys.rl.learning_statistics.LearningStatistics" href="../rl/learning_statistics.html#deeprecsys.rl.learning_statistics.LearningStatistics">LearningStatistics</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dueling DQN with Double DQN and Noisy Networks</p>
<p>Initialize the network with the provided parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DuelingDDQN(BaseNetwork):
    &#34;&#34;&#34;Dueling DQN with Double DQN and Noisy Networks&#34;&#34;&#34;

    def __init__(
        self,
        n_input: int,
        n_output: int,
        learning_rate: float,
        hidden_layers: List[int] = None,
        noise_sigma: float = 0.17,
        discount_factor: float = 0.99,
        statistics: Optional[LearningStatistics] = None,
    ):
        &#34;&#34;&#34;Initialize the network with the provided parameters&#34;&#34;&#34;
        super().__init__()
        if not hidden_layers:
            hidden_layers = [256, 256, 64, 64]
        self.discount_factor = discount_factor
        self._build_network(n_input, n_output, noise_sigma, hidden_layers=hidden_layers)
        self.optimizer = Adam(self.parameters(), lr=learning_rate)
        self.statistics = statistics

    def _build_network(self, n_input: int, n_output: int, noise_sigma: float, hidden_layers: List[int]) -&gt; None:
        &#34;&#34;&#34;Build the dueling network with noisy layers, the value
        subnet and the advantage subnet. TODO: add `.to_device()` to Modules
        &#34;&#34;&#34;
        if len(hidden_layers) != 4:
            raise ValueError(&#34;Unexpected amount of layers&#34;)
        fc_1, fc_2, value_size, advantage_size = hidden_layers
        self.fully_connected_1 = Linear(n_input, fc_1, bias=True)
        self.fully_connected_2 = NoisyLayer(fc_1, fc_2, bias=True, sigma=noise_sigma)
        self.value_subnet = Sequential(
            NoisyLayer(fc_2, value_size, bias=True, sigma=noise_sigma),
            ReLU(),
            Linear(value_size, 1, bias=True),
        )
        self.advantage_subnet = Sequential(
            NoisyLayer(fc_2, advantage_size, bias=True, sigma=noise_sigma),
            ReLU(),
            Linear(advantage_size, n_output, bias=True),
        )

    def forward(self, state: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;Calculate the forward between the layers&#34;&#34;&#34;
        layer_1_out = functional.relu(self.fully_connected_1(state))
        layer_2_out = functional.relu(self.fully_connected_2(layer_1_out))
        value_of_state = self.value_subnet(layer_2_out)
        advantage_of_state = self.advantage_subnet(layer_2_out)
        # This is the Dueling DQN part
        # Combines V and A to get Q: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a&#39;))
        if len(state.shape) == 2:
            q_values = value_of_state + (advantage_of_state - advantage_of_state.mean(dim=1, keepdim=True))
        else:
            q_values = value_of_state + (advantage_of_state - advantage_of_state.mean())
        return q_values

    def get_q_values(self, state: Any) -&gt; Tensor:
        &#34;&#34;&#34;Run the state through the network and return the Q-value for each action.&#34;&#34;&#34;
        if type(state) is tuple:
            state = array([ravel(s) for s in state])
        state_tensor = FloatTensor(state).to(device=self.device)
        return self.forward(state_tensor)

    def top_k_actions_for_state(self, state: Any, k: int = 1) -&gt; List[int]:
        &#34;&#34;&#34;Get the top K actions ranked by their estimated Q-value.&#34;&#34;&#34;
        q_values = self.get_q_values(state)
        _, top_indices = q_values.topk(k=k)
        return [int(v) for v in top_indices.detach().numpy()]  # TODO: cpu() ?

    def learn_with(self, buffer: PrioritizedExperienceReplayBuffer, target_network: Module) -&gt; None:
        &#34;&#34;&#34;Train the target network using the replay buffer.&#34;&#34;&#34;
        experiences = buffer.sample_batch()
        self.optimizer.zero_grad()
        td_error, weights = self._calculate_td_error_and_weigths(experiences, target_network)
        loss = (td_error.pow(2) * weights).mean().to(self.device)
        loss.backward()
        self.optimizer.step()
        # store loss in statistics
        if self.statistics:
            if self.device == &#34;cuda&#34;:
                self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().cpu().numpy()))
            else:
                self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().numpy()))
        # update buffer priorities
        errors_from_batch = td_error.detach().cpu().numpy().flatten()
        buffer.update_priorities(experiences, errors_from_batch)

    def _calculate_td_error_and_weigths(
        self, experiences: List[Tuple], target_network: Module
    ) -&gt; Tuple[Tensor, Tensor]:
        states, actions, rewards, dones, next_states, weights, samples = [i for i in experiences]
        # convert to tensors
        state_tensors = FloatTensor(states).to(device=self.device)
        next_state_tensors = FloatTensor(next_states).to(device=self.device)
        reward_tensors = FloatTensor(rewards).to(device=self.device).reshape(-1, 1)
        action_tensors = LongTensor(array(actions)).reshape(-1, 1).to(device=self.device)
        done_tensors = BoolTensor(dones).to(device=self.device)
        weight_tensors = FloatTensor(weights).to(device=self.device)
        # the following logic is the DDQN update
        # Then we get the predicted actions for the states that came next
        # (using the main network)
        actions_for_next_states = [self.top_k_actions_for_state(s)[0] for s in next_state_tensors]
        actions_for_next_states_tensor = LongTensor(actions_for_next_states).reshape(-1, 1).to(device=self.device)
        # Then we use them to get the estimated Q Values for these next states/actions,
        # according to the target network. Remember that the target network is a copy
        # of this one taken some steps ago
        next_q_values = target_network.forward(next_state_tensors)
        # now we get the q values for the actions that were predicted for the next state
        # we call detach() so no gradient will be backpropagated along this variable
        next_q_values_for_actions = gather(next_q_values, 1, actions_for_next_states_tensor).detach()
        # zero value for done timesteps
        next_q_values_for_actions[done_tensors] = 0
        # bellman equation
        expected_q_values = self.discount_factor * next_q_values_for_actions + reward_tensors
        # Then get the Q-Values of the main network for the selected actions
        q_values = gather(self.forward(state_tensors), 1, action_tensors)
        # And compare them (this is the time-difference or TD error)
        td_error = q_values - expected_q_values
        return td_error, weight_tensors.reshape(-1, 1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.neural_networks.base_network.BaseNetwork" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork">BaseNetwork</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deeprecsys.neural_networks.dueling.DuelingDDQN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, state: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the forward between the layers</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, state: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Calculate the forward between the layers&#34;&#34;&#34;
    layer_1_out = functional.relu(self.fully_connected_1(state))
    layer_2_out = functional.relu(self.fully_connected_2(layer_1_out))
    value_of_state = self.value_subnet(layer_2_out)
    advantage_of_state = self.advantage_subnet(layer_2_out)
    # This is the Dueling DQN part
    # Combines V and A to get Q: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a&#39;))
    if len(state.shape) == 2:
        q_values = value_of_state + (advantage_of_state - advantage_of_state.mean(dim=1, keepdim=True))
    else:
        q_values = value_of_state + (advantage_of_state - advantage_of_state.mean())
    return q_values</code></pre>
</details>
</dd>
<dt id="deeprecsys.neural_networks.dueling.DuelingDDQN.get_q_values"><code class="name flex">
<span>def <span class="ident">get_q_values</span></span>(<span>self, state: Any) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Run the state through the network and return the Q-value for each action.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_q_values(self, state: Any) -&gt; Tensor:
    &#34;&#34;&#34;Run the state through the network and return the Q-value for each action.&#34;&#34;&#34;
    if type(state) is tuple:
        state = array([ravel(s) for s in state])
    state_tensor = FloatTensor(state).to(device=self.device)
    return self.forward(state_tensor)</code></pre>
</details>
</dd>
<dt id="deeprecsys.neural_networks.dueling.DuelingDDQN.learn_with"><code class="name flex">
<span>def <span class="ident">learn_with</span></span>(<span>self, buffer: <a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer" href="../rl/experience_replay/priority_replay_buffer.html#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer">PrioritizedExperienceReplayBuffer</a>, target_network: torch.nn.modules.module.Module) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Train the target network using the replay buffer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_with(self, buffer: PrioritizedExperienceReplayBuffer, target_network: Module) -&gt; None:
    &#34;&#34;&#34;Train the target network using the replay buffer.&#34;&#34;&#34;
    experiences = buffer.sample_batch()
    self.optimizer.zero_grad()
    td_error, weights = self._calculate_td_error_and_weigths(experiences, target_network)
    loss = (td_error.pow(2) * weights).mean().to(self.device)
    loss.backward()
    self.optimizer.step()
    # store loss in statistics
    if self.statistics:
        if self.device == &#34;cuda&#34;:
            self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().cpu().numpy()))
        else:
            self.statistics.append_metric(&#34;loss&#34;, float(loss.detach().numpy()))
    # update buffer priorities
    errors_from_batch = td_error.detach().cpu().numpy().flatten()
    buffer.update_priorities(experiences, errors_from_batch)</code></pre>
</details>
</dd>
<dt id="deeprecsys.neural_networks.dueling.DuelingDDQN.top_k_actions_for_state"><code class="name flex">
<span>def <span class="ident">top_k_actions_for_state</span></span>(<span>self, state: Any, k: int = 1) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the top K actions ranked by their estimated Q-value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def top_k_actions_for_state(self, state: Any, k: int = 1) -&gt; List[int]:
    &#34;&#34;&#34;Get the top K actions ranked by their estimated Q-value.&#34;&#34;&#34;
    q_values = self.get_q_values(state)
    _, top_indices = q_values.topk(k=k)
    return [int(v) for v in top_indices.detach().numpy()]  # TODO: cpu() ?</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.neural_networks.base_network.BaseNetwork" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork">BaseNetwork</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.add_to_tensorboard" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.add_to_tensorboard">add_to_tensorboard</a></code></li>
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.disable_learning" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.disable_learning">disable_learning</a></code></li>
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.load" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.load">load</a></code></li>
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.run_backpropagation" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.run_backpropagation">run_backpropagation</a></code></li>
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.save" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.save">save</a></code></li>
<li><code><a title="deeprecsys.neural_networks.base_network.BaseNetwork.soft_parameter_update" href="base_network.html#deeprecsys.neural_networks.base_network.BaseNetwork.soft_parameter_update">soft_parameter_update</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deeprecsys.neural_networks" href="index.html">deeprecsys.neural_networks</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deeprecsys.neural_networks.dueling.DuelingDDQN" href="#deeprecsys.neural_networks.dueling.DuelingDDQN">DuelingDDQN</a></code></h4>
<ul class="">
<li><code><a title="deeprecsys.neural_networks.dueling.DuelingDDQN.forward" href="#deeprecsys.neural_networks.dueling.DuelingDDQN.forward">forward</a></code></li>
<li><code><a title="deeprecsys.neural_networks.dueling.DuelingDDQN.get_q_values" href="#deeprecsys.neural_networks.dueling.DuelingDDQN.get_q_values">get_q_values</a></code></li>
<li><code><a title="deeprecsys.neural_networks.dueling.DuelingDDQN.learn_with" href="#deeprecsys.neural_networks.dueling.DuelingDDQN.learn_with">learn_with</a></code></li>
<li><code><a title="deeprecsys.neural_networks.dueling.DuelingDDQN.top_k_actions_for_state" href="#deeprecsys.neural_networks.dueling.DuelingDDQN.top_k_actions_for_state">top_k_actions_for_state</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>