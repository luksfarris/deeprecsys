<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>deeprecsys.rl.manager API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deeprecsys.rl.manager</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
from collections import defaultdict, namedtuple
from typing import Any, Generator, List, Optional

import numpy as np
import torch
from gymnasium import Env, make, spec
from numpy.core.multiarray import ndarray
from numpy.random import RandomState

from deeprecsys.rl import Logger
from deeprecsys.rl.agents.agent import ReinforcementLearning
from deeprecsys.rl.learning_statistics import LearningStatistics

# An episode output is a data model to represent 3 things: how many timesteps the
# episode took to finish, the total sum of rewards, and the average reward sum of the
# last 100 episodes.
EpisodeOutput = namedtuple(&#34;EpisodeOutput&#34;, &#34;timesteps,reward_sum&#34;)

logger = Logger.create()


class Manager:
    &#34;&#34;&#34;Class for learning from gym environments with some convenience methods.&#34;&#34;&#34;

    env_name: str
    env: Any
    seed: int | None = None
    random_state: RandomState | None = None

    def __init__(
        self,
        env_name: Optional[str] = None,
        seed: Optional[int] = None,
        env: Optional[Env] = None,
        max_episode_steps: float = math.inf,
        reward_threshold: float = math.inf,
        **kwargs: Any,
    ) -&gt; None:
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        if any([env_name is None and env is None, env_name is not None and env is not None]):
            raise ValueError(&#34;Must specify exactly one of [env_name, env]&#34;)
        if env_name is not None:
            self.env_name = env_name
            # extract some parameters from the environment
            self.max_episode_steps = spec(self.env_name).max_episode_steps or max_episode_steps
            self.reward_threshold = spec(self.env_name).reward_threshold or reward_threshold
            # create the environment
            self.env = make(env_name, **kwargs)
            # we seed the environment so that results are reproducible
        else:
            self.env = env
            self.max_episode_steps = max_episode_steps
            self.reward_threshold = reward_threshold

        self.setup_reproducibility(seed)
        self.slate_size: int = kwargs[&#34;slate_size&#34;] if &#34;slate_size&#34; in kwargs else 1

    def print_overview(self) -&gt; None:
        &#34;&#34;&#34;Print the most important variables of the environment.&#34;&#34;&#34;
        logger.info(&#34;Reward threshold: {} &#34;.format(self.reward_threshold))
        logger.info(&#34;Reward signal range: {} &#34;.format(self.env.reward_range))
        logger.info(&#34;Maximum episode steps: {} &#34;.format(self.max_episode_steps))
        logger.info(&#34;Action apace size: {}&#34;.format(self.env.action_space))
        logger.info(&#34;Observation space size {} &#34;.format(self.env.observation_space))

    def execute_episodes(
        self,
        rl: ReinforcementLearning,
        n_episodes: int = 1,
        should_render: bool = False,
    ) -&gt; List[EpisodeOutput]:
        &#34;&#34;&#34;Execute any number of episodes with the given agent.
        Returns the number of timesteps and sum of rewards per episode.
        &#34;&#34;&#34;
        episode_outputs = []
        for episode in range(n_episodes):
            t, reward_sum, done, (state, _) = 0, 0, False, self.env.reset(seed=self.seed)
            logger.info(f&#34;Running episode {episode}, starting at state {state}&#34;)
            while not done and t &lt; self.max_episode_steps:
                if should_render:
                    self.env.render()
                action = rl.action_for_state(state)
                state, reward, done, _ = self.env.step(action)
                logger.info(f&#34;t={t} a={action} r={reward} s={state}&#34;)
                reward_sum += reward
                t += 1
            episode_outputs.append(EpisodeOutput(t, reward_sum))
            self.env.close()
        return episode_outputs

    @staticmethod
    def _train_start_new_episode(statistics: LearningStatistics, episode: int) -&gt; None:
        if statistics:
            statistics.episode = episode
            statistics.timestep = 0

    @staticmethod
    def _train_update_timestep(statistics: LearningStatistics) -&gt; None:
        if statistics:
            statistics.timestep += 1

    @staticmethod
    def _train_add_statistics(statistics: LearningStatistics, rewards: List, moving_average: ndarray) -&gt; None:
        if statistics:
            statistics.append_metric(&#34;episode_rewards&#34;, sum(rewards))
            statistics.append_metric(&#34;timestep_rewards&#34;, rewards)
            statistics.append_metric(&#34;moving_rewards&#34;, moving_average)

    def _train_get_step_action(self, rl: ReinforcementLearning, state: Any) -&gt; Any:
        if self.slate_size == 1:
            return rl.action_for_state(state)
        else:
            return rl.top_k_actions_for_state(state, k=self.slate_size)

    def train(
        self,
        rl: ReinforcementLearning,
        statistics: Optional[LearningStatistics] = None,
        max_episodes: int = 50,
    ) -&gt; None:
        &#34;&#34;&#34;Train the agent for the given amount of episodes.&#34;&#34;&#34;
        logger.info(&#34;Training...&#34;)
        episode_rewards = []
        for episode in range(max_episodes):
            state, info = self.env.reset(seed=self.seed)
            rewards = []
            self._train_start_new_episode(statistics, episode)
            done = False
            while done is False:
                action = self._train_get_step_action(rl, state)
                new_state, reward, done, _, info = self.env.step(action)
                if &#34;chosen_action&#34; in info:
                    action = action[info[&#34;chosen_action&#34;]]
                rl.store_experience(state, action, reward, done, new_state)
                rewards.append(reward)
                state = new_state.copy()
                self._train_update_timestep(statistics)
            episode_rewards.append(sum(rewards))
            moving_average = np.mean(episode_rewards[-100:])
            self._train_add_statistics(statistics, rewards, moving_average)

            logger.print(
                f&#34;\rEpisode {episode:d} Mean Rewards {moving_average:.2f} Last Reward {rewards[-1]:.2f}\t\t&#34;,
                end=&#34;&#34;,
            )
            if moving_average &gt;= self.reward_threshold:
                logger.info(&#34;Reward threshold reached&#34;)
                break

    def _hyperparameter_search_run_combinations(
        self,
        runs_per_combination: int,
        rl: ReinforcementLearning,
        episodes: int,
        learning_statistics: LearningStatistics,
        parameter_name: str,
        parameter_value: Any,
    ) -&gt; Generator:
        for run in range(runs_per_combination):
            self.train(
                rl=rl,
                max_episodes=episodes,
                statistics=learning_statistics,
            )
            yield learning_statistics.moving_rewards.iloc[-1]
            logger.print(
                f&#34;\rTested combination {parameter_name}={parameter_value} round {run} result was {learning_statistics.moving_rewards.iloc[-1]} \t\t&#34;,  # noqa: E501
                end=&#34;&#34;,
            )

    def hyperparameter_search(
        self,
        agent: type,
        params: dict,
        default_params: dict,
        episodes: int = 100,
        runs_per_combination: int = 3,
    ) -&gt; dict:
        &#34;&#34;&#34;Given an agent class, and a dictionary of hyperparameter names and values,
        will try all combinations, and return the mean reward of each combination
        for the given number of episodes, and will run the determined number of times.
        &#34;&#34;&#34;
        combination_results = defaultdict(lambda: [])
        for p_name, p_value in params.items():
            if len(p_value) &lt; 2:
                continue
            for value in p_value:
                rl = agent(**{**default_params, p_name: value})
                learning_statistics = LearningStatistics()
                combination_key = f&#34;{p_name}={value}&#34;
                for result in self._hyperparameter_search_run_combinations(
                    runs_per_combination,
                    rl,
                    episodes,
                    learning_statistics,
                    p_name,
                    value,
                ):
                    combination_results[combination_key].append(result)

        return combination_results

    def setup_reproducibility(self, seed: Optional[int] = None) -&gt; Optional[RandomState]:
        &#34;&#34;&#34;Seeds the project&#39;s libraries: numpy, torch, gym&#34;&#34;&#34;
        if seed:
            # seed pytorch
            torch.manual_seed(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            # seed numpy
            np.random.seed(seed)
            # seed gym
            self.seed = seed
            self.random_state = RandomState(seed)
            return self.random_state
        return None


class HighwayManager(Manager):
    &#34;&#34;&#34;Manager for the highway environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None, vehicles: int = 50):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;highway-v0&#34;, seed=seed)
        self.env.configure({&#34;vehicles_count&#34;: vehicles})
        self.max_episode_steps = self.env.config[&#34;duration&#34;]


class CartpoleManager(Manager):
    &#34;&#34;&#34;Manager for the cart pole environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;CartPole-v1&#34;, seed=seed)
        self.reward_threshold = 50


class LunarLanderManager(Manager):
    &#34;&#34;&#34;Manager for the lunar lander environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;LunarLander-v2&#34;, seed=seed)


class MovieLensFairnessManager(Manager):
    &#34;&#34;&#34;Manager for the movie lens environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None, slate_size: int = 1):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;MovieLensFairness-v0&#34;, seed=seed, slate_size=slate_size)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deeprecsys.rl.manager.CartpoleManager"><code class="flex name class">
<span>class <span class="ident">CartpoleManager</span></span>
<span>(</span><span>seed: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Manager for the cart pole environment</p>
<p>Start the manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CartpoleManager(Manager):
    &#34;&#34;&#34;Manager for the cart pole environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;CartPole-v1&#34;, seed=seed)
        self.reward_threshold = 50</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.rl.manager.Manager.execute_episodes" href="#deeprecsys.rl.manager.Manager.execute_episodes">execute_episodes</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.hyperparameter_search" href="#deeprecsys.rl.manager.Manager.hyperparameter_search">hyperparameter_search</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.print_overview" href="#deeprecsys.rl.manager.Manager.print_overview">print_overview</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.setup_reproducibility" href="#deeprecsys.rl.manager.Manager.setup_reproducibility">setup_reproducibility</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.train" href="#deeprecsys.rl.manager.Manager.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deeprecsys.rl.manager.EpisodeOutput"><code class="flex name class">
<span>class <span class="ident">EpisodeOutput</span></span>
<span>(</span><span>timesteps, reward_sum)</span>
</code></dt>
<dd>
<div class="desc"><p>EpisodeOutput(timesteps, reward_sum)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="deeprecsys.rl.manager.EpisodeOutput.reward_sum"><code class="name">var <span class="ident">reward_sum</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="deeprecsys.rl.manager.EpisodeOutput.timesteps"><code class="name">var <span class="ident">timesteps</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
</dl>
</dd>
<dt id="deeprecsys.rl.manager.HighwayManager"><code class="flex name class">
<span>class <span class="ident">HighwayManager</span></span>
<span>(</span><span>seed: Optional[int] = None, vehicles: int = 50)</span>
</code></dt>
<dd>
<div class="desc"><p>Manager for the highway environment</p>
<p>Start the manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HighwayManager(Manager):
    &#34;&#34;&#34;Manager for the highway environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None, vehicles: int = 50):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;highway-v0&#34;, seed=seed)
        self.env.configure({&#34;vehicles_count&#34;: vehicles})
        self.max_episode_steps = self.env.config[&#34;duration&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.rl.manager.Manager.execute_episodes" href="#deeprecsys.rl.manager.Manager.execute_episodes">execute_episodes</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.hyperparameter_search" href="#deeprecsys.rl.manager.Manager.hyperparameter_search">hyperparameter_search</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.print_overview" href="#deeprecsys.rl.manager.Manager.print_overview">print_overview</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.setup_reproducibility" href="#deeprecsys.rl.manager.Manager.setup_reproducibility">setup_reproducibility</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.train" href="#deeprecsys.rl.manager.Manager.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deeprecsys.rl.manager.LunarLanderManager"><code class="flex name class">
<span>class <span class="ident">LunarLanderManager</span></span>
<span>(</span><span>seed: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Manager for the lunar lander environment</p>
<p>Start the manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LunarLanderManager(Manager):
    &#34;&#34;&#34;Manager for the lunar lander environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;LunarLander-v2&#34;, seed=seed)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.rl.manager.Manager.execute_episodes" href="#deeprecsys.rl.manager.Manager.execute_episodes">execute_episodes</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.hyperparameter_search" href="#deeprecsys.rl.manager.Manager.hyperparameter_search">hyperparameter_search</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.print_overview" href="#deeprecsys.rl.manager.Manager.print_overview">print_overview</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.setup_reproducibility" href="#deeprecsys.rl.manager.Manager.setup_reproducibility">setup_reproducibility</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.train" href="#deeprecsys.rl.manager.Manager.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deeprecsys.rl.manager.Manager"><code class="flex name class">
<span>class <span class="ident">Manager</span></span>
<span>(</span><span>env_name: Optional[str] = None, seed: Optional[int] = None, env: Optional[gymnasium.core.Env] = None, max_episode_steps: float = inf, reward_threshold: float = inf, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for learning from gym environments with some convenience methods.</p>
<p>Start the manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Manager:
    &#34;&#34;&#34;Class for learning from gym environments with some convenience methods.&#34;&#34;&#34;

    env_name: str
    env: Any
    seed: int | None = None
    random_state: RandomState | None = None

    def __init__(
        self,
        env_name: Optional[str] = None,
        seed: Optional[int] = None,
        env: Optional[Env] = None,
        max_episode_steps: float = math.inf,
        reward_threshold: float = math.inf,
        **kwargs: Any,
    ) -&gt; None:
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        if any([env_name is None and env is None, env_name is not None and env is not None]):
            raise ValueError(&#34;Must specify exactly one of [env_name, env]&#34;)
        if env_name is not None:
            self.env_name = env_name
            # extract some parameters from the environment
            self.max_episode_steps = spec(self.env_name).max_episode_steps or max_episode_steps
            self.reward_threshold = spec(self.env_name).reward_threshold or reward_threshold
            # create the environment
            self.env = make(env_name, **kwargs)
            # we seed the environment so that results are reproducible
        else:
            self.env = env
            self.max_episode_steps = max_episode_steps
            self.reward_threshold = reward_threshold

        self.setup_reproducibility(seed)
        self.slate_size: int = kwargs[&#34;slate_size&#34;] if &#34;slate_size&#34; in kwargs else 1

    def print_overview(self) -&gt; None:
        &#34;&#34;&#34;Print the most important variables of the environment.&#34;&#34;&#34;
        logger.info(&#34;Reward threshold: {} &#34;.format(self.reward_threshold))
        logger.info(&#34;Reward signal range: {} &#34;.format(self.env.reward_range))
        logger.info(&#34;Maximum episode steps: {} &#34;.format(self.max_episode_steps))
        logger.info(&#34;Action apace size: {}&#34;.format(self.env.action_space))
        logger.info(&#34;Observation space size {} &#34;.format(self.env.observation_space))

    def execute_episodes(
        self,
        rl: ReinforcementLearning,
        n_episodes: int = 1,
        should_render: bool = False,
    ) -&gt; List[EpisodeOutput]:
        &#34;&#34;&#34;Execute any number of episodes with the given agent.
        Returns the number of timesteps and sum of rewards per episode.
        &#34;&#34;&#34;
        episode_outputs = []
        for episode in range(n_episodes):
            t, reward_sum, done, (state, _) = 0, 0, False, self.env.reset(seed=self.seed)
            logger.info(f&#34;Running episode {episode}, starting at state {state}&#34;)
            while not done and t &lt; self.max_episode_steps:
                if should_render:
                    self.env.render()
                action = rl.action_for_state(state)
                state, reward, done, _ = self.env.step(action)
                logger.info(f&#34;t={t} a={action} r={reward} s={state}&#34;)
                reward_sum += reward
                t += 1
            episode_outputs.append(EpisodeOutput(t, reward_sum))
            self.env.close()
        return episode_outputs

    @staticmethod
    def _train_start_new_episode(statistics: LearningStatistics, episode: int) -&gt; None:
        if statistics:
            statistics.episode = episode
            statistics.timestep = 0

    @staticmethod
    def _train_update_timestep(statistics: LearningStatistics) -&gt; None:
        if statistics:
            statistics.timestep += 1

    @staticmethod
    def _train_add_statistics(statistics: LearningStatistics, rewards: List, moving_average: ndarray) -&gt; None:
        if statistics:
            statistics.append_metric(&#34;episode_rewards&#34;, sum(rewards))
            statistics.append_metric(&#34;timestep_rewards&#34;, rewards)
            statistics.append_metric(&#34;moving_rewards&#34;, moving_average)

    def _train_get_step_action(self, rl: ReinforcementLearning, state: Any) -&gt; Any:
        if self.slate_size == 1:
            return rl.action_for_state(state)
        else:
            return rl.top_k_actions_for_state(state, k=self.slate_size)

    def train(
        self,
        rl: ReinforcementLearning,
        statistics: Optional[LearningStatistics] = None,
        max_episodes: int = 50,
    ) -&gt; None:
        &#34;&#34;&#34;Train the agent for the given amount of episodes.&#34;&#34;&#34;
        logger.info(&#34;Training...&#34;)
        episode_rewards = []
        for episode in range(max_episodes):
            state, info = self.env.reset(seed=self.seed)
            rewards = []
            self._train_start_new_episode(statistics, episode)
            done = False
            while done is False:
                action = self._train_get_step_action(rl, state)
                new_state, reward, done, _, info = self.env.step(action)
                if &#34;chosen_action&#34; in info:
                    action = action[info[&#34;chosen_action&#34;]]
                rl.store_experience(state, action, reward, done, new_state)
                rewards.append(reward)
                state = new_state.copy()
                self._train_update_timestep(statistics)
            episode_rewards.append(sum(rewards))
            moving_average = np.mean(episode_rewards[-100:])
            self._train_add_statistics(statistics, rewards, moving_average)

            logger.print(
                f&#34;\rEpisode {episode:d} Mean Rewards {moving_average:.2f} Last Reward {rewards[-1]:.2f}\t\t&#34;,
                end=&#34;&#34;,
            )
            if moving_average &gt;= self.reward_threshold:
                logger.info(&#34;Reward threshold reached&#34;)
                break

    def _hyperparameter_search_run_combinations(
        self,
        runs_per_combination: int,
        rl: ReinforcementLearning,
        episodes: int,
        learning_statistics: LearningStatistics,
        parameter_name: str,
        parameter_value: Any,
    ) -&gt; Generator:
        for run in range(runs_per_combination):
            self.train(
                rl=rl,
                max_episodes=episodes,
                statistics=learning_statistics,
            )
            yield learning_statistics.moving_rewards.iloc[-1]
            logger.print(
                f&#34;\rTested combination {parameter_name}={parameter_value} round {run} result was {learning_statistics.moving_rewards.iloc[-1]} \t\t&#34;,  # noqa: E501
                end=&#34;&#34;,
            )

    def hyperparameter_search(
        self,
        agent: type,
        params: dict,
        default_params: dict,
        episodes: int = 100,
        runs_per_combination: int = 3,
    ) -&gt; dict:
        &#34;&#34;&#34;Given an agent class, and a dictionary of hyperparameter names and values,
        will try all combinations, and return the mean reward of each combination
        for the given number of episodes, and will run the determined number of times.
        &#34;&#34;&#34;
        combination_results = defaultdict(lambda: [])
        for p_name, p_value in params.items():
            if len(p_value) &lt; 2:
                continue
            for value in p_value:
                rl = agent(**{**default_params, p_name: value})
                learning_statistics = LearningStatistics()
                combination_key = f&#34;{p_name}={value}&#34;
                for result in self._hyperparameter_search_run_combinations(
                    runs_per_combination,
                    rl,
                    episodes,
                    learning_statistics,
                    p_name,
                    value,
                ):
                    combination_results[combination_key].append(result)

        return combination_results

    def setup_reproducibility(self, seed: Optional[int] = None) -&gt; Optional[RandomState]:
        &#34;&#34;&#34;Seeds the project&#39;s libraries: numpy, torch, gym&#34;&#34;&#34;
        if seed:
            # seed pytorch
            torch.manual_seed(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            # seed numpy
            np.random.seed(seed)
            # seed gym
            self.seed = seed
            self.random_state = RandomState(seed)
            return self.random_state
        return None</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.manager.CartpoleManager" href="#deeprecsys.rl.manager.CartpoleManager">CartpoleManager</a></li>
<li><a title="deeprecsys.rl.manager.HighwayManager" href="#deeprecsys.rl.manager.HighwayManager">HighwayManager</a></li>
<li><a title="deeprecsys.rl.manager.LunarLanderManager" href="#deeprecsys.rl.manager.LunarLanderManager">LunarLanderManager</a></li>
<li><a title="deeprecsys.rl.manager.MovieLensFairnessManager" href="#deeprecsys.rl.manager.MovieLensFairnessManager">MovieLensFairnessManager</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="deeprecsys.rl.manager.Manager.env"><code class="name">var <span class="ident">env</span> : Any</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="deeprecsys.rl.manager.Manager.env_name"><code class="name">var <span class="ident">env_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="deeprecsys.rl.manager.Manager.random_state"><code class="name">var <span class="ident">random_state</span> : Optional[numpy.random.mtrand.RandomState]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="deeprecsys.rl.manager.Manager.seed"><code class="name">var <span class="ident">seed</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="deeprecsys.rl.manager.Manager.execute_episodes"><code class="name flex">
<span>def <span class="ident">execute_episodes</span></span>(<span>self, rl: <a title="deeprecsys.rl.agents.agent.ReinforcementLearning" href="agents/agent.html#deeprecsys.rl.agents.agent.ReinforcementLearning">ReinforcementLearning</a>, n_episodes: int = 1, should_render: bool = False) ‑> List[<a title="deeprecsys.rl.manager.EpisodeOutput" href="#deeprecsys.rl.manager.EpisodeOutput">EpisodeOutput</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Execute any number of episodes with the given agent.
Returns the number of timesteps and sum of rewards per episode.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_episodes(
    self,
    rl: ReinforcementLearning,
    n_episodes: int = 1,
    should_render: bool = False,
) -&gt; List[EpisodeOutput]:
    &#34;&#34;&#34;Execute any number of episodes with the given agent.
    Returns the number of timesteps and sum of rewards per episode.
    &#34;&#34;&#34;
    episode_outputs = []
    for episode in range(n_episodes):
        t, reward_sum, done, (state, _) = 0, 0, False, self.env.reset(seed=self.seed)
        logger.info(f&#34;Running episode {episode}, starting at state {state}&#34;)
        while not done and t &lt; self.max_episode_steps:
            if should_render:
                self.env.render()
            action = rl.action_for_state(state)
            state, reward, done, _ = self.env.step(action)
            logger.info(f&#34;t={t} a={action} r={reward} s={state}&#34;)
            reward_sum += reward
            t += 1
        episode_outputs.append(EpisodeOutput(t, reward_sum))
        self.env.close()
    return episode_outputs</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.manager.Manager.hyperparameter_search"><code class="name flex">
<span>def <span class="ident">hyperparameter_search</span></span>(<span>self, agent: type, params: dict, default_params: dict, episodes: int = 100, runs_per_combination: int = 3) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Given an agent class, and a dictionary of hyperparameter names and values,
will try all combinations, and return the mean reward of each combination
for the given number of episodes, and will run the determined number of times.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hyperparameter_search(
    self,
    agent: type,
    params: dict,
    default_params: dict,
    episodes: int = 100,
    runs_per_combination: int = 3,
) -&gt; dict:
    &#34;&#34;&#34;Given an agent class, and a dictionary of hyperparameter names and values,
    will try all combinations, and return the mean reward of each combination
    for the given number of episodes, and will run the determined number of times.
    &#34;&#34;&#34;
    combination_results = defaultdict(lambda: [])
    for p_name, p_value in params.items():
        if len(p_value) &lt; 2:
            continue
        for value in p_value:
            rl = agent(**{**default_params, p_name: value})
            learning_statistics = LearningStatistics()
            combination_key = f&#34;{p_name}={value}&#34;
            for result in self._hyperparameter_search_run_combinations(
                runs_per_combination,
                rl,
                episodes,
                learning_statistics,
                p_name,
                value,
            ):
                combination_results[combination_key].append(result)

    return combination_results</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.manager.Manager.print_overview"><code class="name flex">
<span>def <span class="ident">print_overview</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Print the most important variables of the environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_overview(self) -&gt; None:
    &#34;&#34;&#34;Print the most important variables of the environment.&#34;&#34;&#34;
    logger.info(&#34;Reward threshold: {} &#34;.format(self.reward_threshold))
    logger.info(&#34;Reward signal range: {} &#34;.format(self.env.reward_range))
    logger.info(&#34;Maximum episode steps: {} &#34;.format(self.max_episode_steps))
    logger.info(&#34;Action apace size: {}&#34;.format(self.env.action_space))
    logger.info(&#34;Observation space size {} &#34;.format(self.env.observation_space))</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.manager.Manager.setup_reproducibility"><code class="name flex">
<span>def <span class="ident">setup_reproducibility</span></span>(<span>self, seed: Optional[int] = None) ‑> Optional[numpy.random.mtrand.RandomState]</span>
</code></dt>
<dd>
<div class="desc"><p>Seeds the project's libraries: numpy, torch, gym</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_reproducibility(self, seed: Optional[int] = None) -&gt; Optional[RandomState]:
    &#34;&#34;&#34;Seeds the project&#39;s libraries: numpy, torch, gym&#34;&#34;&#34;
    if seed:
        # seed pytorch
        torch.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # seed numpy
        np.random.seed(seed)
        # seed gym
        self.seed = seed
        self.random_state = RandomState(seed)
        return self.random_state
    return None</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.manager.Manager.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, rl: <a title="deeprecsys.rl.agents.agent.ReinforcementLearning" href="agents/agent.html#deeprecsys.rl.agents.agent.ReinforcementLearning">ReinforcementLearning</a>, statistics: Optional[<a title="deeprecsys.rl.learning_statistics.LearningStatistics" href="learning_statistics.html#deeprecsys.rl.learning_statistics.LearningStatistics">LearningStatistics</a>] = None, max_episodes: int = 50) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Train the agent for the given amount of episodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(
    self,
    rl: ReinforcementLearning,
    statistics: Optional[LearningStatistics] = None,
    max_episodes: int = 50,
) -&gt; None:
    &#34;&#34;&#34;Train the agent for the given amount of episodes.&#34;&#34;&#34;
    logger.info(&#34;Training...&#34;)
    episode_rewards = []
    for episode in range(max_episodes):
        state, info = self.env.reset(seed=self.seed)
        rewards = []
        self._train_start_new_episode(statistics, episode)
        done = False
        while done is False:
            action = self._train_get_step_action(rl, state)
            new_state, reward, done, _, info = self.env.step(action)
            if &#34;chosen_action&#34; in info:
                action = action[info[&#34;chosen_action&#34;]]
            rl.store_experience(state, action, reward, done, new_state)
            rewards.append(reward)
            state = new_state.copy()
            self._train_update_timestep(statistics)
        episode_rewards.append(sum(rewards))
        moving_average = np.mean(episode_rewards[-100:])
        self._train_add_statistics(statistics, rewards, moving_average)

        logger.print(
            f&#34;\rEpisode {episode:d} Mean Rewards {moving_average:.2f} Last Reward {rewards[-1]:.2f}\t\t&#34;,
            end=&#34;&#34;,
        )
        if moving_average &gt;= self.reward_threshold:
            logger.info(&#34;Reward threshold reached&#34;)
            break</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deeprecsys.rl.manager.MovieLensFairnessManager"><code class="flex name class">
<span>class <span class="ident">MovieLensFairnessManager</span></span>
<span>(</span><span>seed: Optional[int] = None, slate_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Manager for the movie lens environment</p>
<p>Start the manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MovieLensFairnessManager(Manager):
    &#34;&#34;&#34;Manager for the movie lens environment&#34;&#34;&#34;

    def __init__(self, seed: Optional[int] = None, slate_size: int = 1):
        &#34;&#34;&#34;Start the manager&#34;&#34;&#34;
        super().__init__(env_name=&#34;MovieLensFairness-v0&#34;, seed=seed, slate_size=slate_size)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.rl.manager.Manager.execute_episodes" href="#deeprecsys.rl.manager.Manager.execute_episodes">execute_episodes</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.hyperparameter_search" href="#deeprecsys.rl.manager.Manager.hyperparameter_search">hyperparameter_search</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.print_overview" href="#deeprecsys.rl.manager.Manager.print_overview">print_overview</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.setup_reproducibility" href="#deeprecsys.rl.manager.Manager.setup_reproducibility">setup_reproducibility</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.train" href="#deeprecsys.rl.manager.Manager.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deeprecsys.rl" href="index.html">deeprecsys.rl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deeprecsys.rl.manager.CartpoleManager" href="#deeprecsys.rl.manager.CartpoleManager">CartpoleManager</a></code></h4>
</li>
<li>
<h4><code><a title="deeprecsys.rl.manager.EpisodeOutput" href="#deeprecsys.rl.manager.EpisodeOutput">EpisodeOutput</a></code></h4>
<ul class="">
<li><code><a title="deeprecsys.rl.manager.EpisodeOutput.reward_sum" href="#deeprecsys.rl.manager.EpisodeOutput.reward_sum">reward_sum</a></code></li>
<li><code><a title="deeprecsys.rl.manager.EpisodeOutput.timesteps" href="#deeprecsys.rl.manager.EpisodeOutput.timesteps">timesteps</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deeprecsys.rl.manager.HighwayManager" href="#deeprecsys.rl.manager.HighwayManager">HighwayManager</a></code></h4>
</li>
<li>
<h4><code><a title="deeprecsys.rl.manager.LunarLanderManager" href="#deeprecsys.rl.manager.LunarLanderManager">LunarLanderManager</a></code></h4>
</li>
<li>
<h4><code><a title="deeprecsys.rl.manager.Manager" href="#deeprecsys.rl.manager.Manager">Manager</a></code></h4>
<ul class="">
<li><code><a title="deeprecsys.rl.manager.Manager.env" href="#deeprecsys.rl.manager.Manager.env">env</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.env_name" href="#deeprecsys.rl.manager.Manager.env_name">env_name</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.execute_episodes" href="#deeprecsys.rl.manager.Manager.execute_episodes">execute_episodes</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.hyperparameter_search" href="#deeprecsys.rl.manager.Manager.hyperparameter_search">hyperparameter_search</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.print_overview" href="#deeprecsys.rl.manager.Manager.print_overview">print_overview</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.random_state" href="#deeprecsys.rl.manager.Manager.random_state">random_state</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.seed" href="#deeprecsys.rl.manager.Manager.seed">seed</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.setup_reproducibility" href="#deeprecsys.rl.manager.Manager.setup_reproducibility">setup_reproducibility</a></code></li>
<li><code><a title="deeprecsys.rl.manager.Manager.train" href="#deeprecsys.rl.manager.Manager.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deeprecsys.rl.manager.MovieLensFairnessManager" href="#deeprecsys.rl.manager.MovieLensFairnessManager">MovieLensFairnessManager</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>