<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>deeprecsys.rl.experience_replay.priority_replay_buffer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deeprecsys.rl.experience_replay.priority_replay_buffer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from collections import namedtuple
from typing import Any, List, Tuple

import numpy
import numpy as np

from deeprecsys.rl.experience_replay.buffer_parameters import (
    ExperienceReplayBufferParameters,
    PERBufferParameters,
)
from deeprecsys.rl.experience_replay.experience_buffer import (
    Experience,
    ExperienceReplayBuffer,
)

PriorityExperience = namedtuple(  # type: ignore
    &#34;PriorityExperience&#34;, field_names=[&#34;experience&#34;, &#34;priority&#34;]
)


class PrioritizedExperienceReplayBuffer(ExperienceReplayBuffer):
    &#34;&#34;&#34;Experience Replay Buffer that gives priority to experiences that the network learns more from. We can tell this
    using the loss. We use importance sampling to avoid bias towards those experiences.
    &#34;&#34;&#34;

    def __init__(
        self,
        buffer_parameters: ExperienceReplayBufferParameters = None,
        per_parameters: PERBufferParameters = None,
    ):
        &#34;&#34;&#34;Start the buffer with the provided parameters.&#34;&#34;&#34;
        if not buffer_parameters:
            buffer_parameters = ExperienceReplayBufferParameters()
        if not per_parameters:
            per_parameters = PERBufferParameters()
        super().__init__(buffer_parameters)
        # beta controls the effect of the weights (how much to learn from each
        # experience in the batch)
        self.beta = per_parameters.beta
        self.beta_growth = per_parameters.beta_growth
        # alpha controls the effect of the priority (how much priority is affected
        # by the loss)
        self.alpha = per_parameters.alpha
        # epsilon guarantees no experience has priority zero
        self.epsilon = per_parameters.epsilon

    def priorities(self) -&gt; numpy.array:
        &#34;&#34;&#34;Get the priority of each experience in the queue&#34;&#34;&#34;
        return numpy.array([e.priority for e in self.experience_queue], dtype=numpy.float32)

    def store_experience(self, state: Any, action: Any, reward: float, done: bool, next_state: Any) -&gt; None:
        &#34;&#34;&#34;We include a priority to the experience. if the queue is empty, priority is 1 (max),
        otherwise we check the maximum priority in the queue
        &#34;&#34;&#34;
        priorities = self.priorities()
        priority = priorities.max() if len(priorities) &gt; 0 else 1.0
        if not np.isnan(priority):
            experience = Experience(state, action, reward, done, next_state)
            priority_experience = PriorityExperience(experience, priority)  # type: ignore
            # append to the right (end) of the queue
            self.experience_queue.append(priority_experience)

    def update_beta(self) -&gt; None:
        &#34;&#34;&#34;We want to grow the beta value slowly and linearly, starting at a value
        close to zero, and stopping at 1.0. This is for the Importance Sampling
        &#34;&#34;&#34;
        if self.beta &lt; 1.0:
            self.beta += self.beta_growth

    def update_priorities(self, batch: List[Tuple], errors_from_batch: List[float]) -&gt; None:
        &#34;&#34;&#34;We want the priority of elements to be the TD error of plus an epsilon
        constant. The epsilon constant makes sure that no experience ever gets a
        priority zero. This prioritization strategy gives more importance to
        elements that bring more learning to the network.
        &#34;&#34;&#34;
        experience_indexes = [b[-1] for b in numpy.array(batch, dtype=&#34;object&#34;).T]
        for i in range(len(experience_indexes)):
            error = abs(errors_from_batch[i]) + self.epsilon
            if not np.isnan(error):
                experience = self.experience_queue[experience_indexes[i]]
                experience._replace(priority=error)
                self.experience_queue[experience_indexes[i]] = experience

    def sample_batch(self) -&gt; List[Tuple]:
        &#34;&#34;&#34;We sample experiences using their priorities as weights for sampling. The
        effect of the priorities is controlled by the alpha parameter. This is
        already an advantage, but it can introduce bias in a network by always
        choosing the same type of experiences for training. In order to fight this, we
        compute the weight of the experience (this is called Importance Sampling,
        or IP). We want the weights to decrease over time, this is controlled by
        the beta parameter.
        &#34;&#34;&#34;
        # calculate probabilities (alpha)
        probabilities = self.priorities() ** self.alpha
        p = probabilities / probabilities.sum()
        # sample experiences
        buffer_size = len(self.experience_queue)
        samples = numpy.random.choice(a=buffer_size, size=self.batch_size, p=p, replace=False)
        experiences = [self.experience_queue[i].experience for i in samples]
        # importance Sampling
        # w_i = (1/N * 1/P_i) ^ beta
        weights = ((1 / buffer_size) * (1 / p[samples])) ** self.beta
        weights = weights / weights.max()
        self.update_beta()
        # return experiences with weights
        return list(zip(*experiences, strict=False)) + [tuple(weights)] + [tuple(samples)]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer"><code class="flex name class">
<span>class <span class="ident">PrioritizedExperienceReplayBuffer</span></span>
<span>(</span><span>buffer_parameters: <a title="deeprecsys.rl.experience_replay.buffer_parameters.ExperienceReplayBufferParameters" href="buffer_parameters.html#deeprecsys.rl.experience_replay.buffer_parameters.ExperienceReplayBufferParameters">ExperienceReplayBufferParameters</a> = None, per_parameters: <a title="deeprecsys.rl.experience_replay.buffer_parameters.PERBufferParameters" href="buffer_parameters.html#deeprecsys.rl.experience_replay.buffer_parameters.PERBufferParameters">PERBufferParameters</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Experience Replay Buffer that gives priority to experiences that the network learns more from. We can tell this
using the loss. We use importance sampling to avoid bias towards those experiences.</p>
<p>Start the buffer with the provided parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PrioritizedExperienceReplayBuffer(ExperienceReplayBuffer):
    &#34;&#34;&#34;Experience Replay Buffer that gives priority to experiences that the network learns more from. We can tell this
    using the loss. We use importance sampling to avoid bias towards those experiences.
    &#34;&#34;&#34;

    def __init__(
        self,
        buffer_parameters: ExperienceReplayBufferParameters = None,
        per_parameters: PERBufferParameters = None,
    ):
        &#34;&#34;&#34;Start the buffer with the provided parameters.&#34;&#34;&#34;
        if not buffer_parameters:
            buffer_parameters = ExperienceReplayBufferParameters()
        if not per_parameters:
            per_parameters = PERBufferParameters()
        super().__init__(buffer_parameters)
        # beta controls the effect of the weights (how much to learn from each
        # experience in the batch)
        self.beta = per_parameters.beta
        self.beta_growth = per_parameters.beta_growth
        # alpha controls the effect of the priority (how much priority is affected
        # by the loss)
        self.alpha = per_parameters.alpha
        # epsilon guarantees no experience has priority zero
        self.epsilon = per_parameters.epsilon

    def priorities(self) -&gt; numpy.array:
        &#34;&#34;&#34;Get the priority of each experience in the queue&#34;&#34;&#34;
        return numpy.array([e.priority for e in self.experience_queue], dtype=numpy.float32)

    def store_experience(self, state: Any, action: Any, reward: float, done: bool, next_state: Any) -&gt; None:
        &#34;&#34;&#34;We include a priority to the experience. if the queue is empty, priority is 1 (max),
        otherwise we check the maximum priority in the queue
        &#34;&#34;&#34;
        priorities = self.priorities()
        priority = priorities.max() if len(priorities) &gt; 0 else 1.0
        if not np.isnan(priority):
            experience = Experience(state, action, reward, done, next_state)
            priority_experience = PriorityExperience(experience, priority)  # type: ignore
            # append to the right (end) of the queue
            self.experience_queue.append(priority_experience)

    def update_beta(self) -&gt; None:
        &#34;&#34;&#34;We want to grow the beta value slowly and linearly, starting at a value
        close to zero, and stopping at 1.0. This is for the Importance Sampling
        &#34;&#34;&#34;
        if self.beta &lt; 1.0:
            self.beta += self.beta_growth

    def update_priorities(self, batch: List[Tuple], errors_from_batch: List[float]) -&gt; None:
        &#34;&#34;&#34;We want the priority of elements to be the TD error of plus an epsilon
        constant. The epsilon constant makes sure that no experience ever gets a
        priority zero. This prioritization strategy gives more importance to
        elements that bring more learning to the network.
        &#34;&#34;&#34;
        experience_indexes = [b[-1] for b in numpy.array(batch, dtype=&#34;object&#34;).T]
        for i in range(len(experience_indexes)):
            error = abs(errors_from_batch[i]) + self.epsilon
            if not np.isnan(error):
                experience = self.experience_queue[experience_indexes[i]]
                experience._replace(priority=error)
                self.experience_queue[experience_indexes[i]] = experience

    def sample_batch(self) -&gt; List[Tuple]:
        &#34;&#34;&#34;We sample experiences using their priorities as weights for sampling. The
        effect of the priorities is controlled by the alpha parameter. This is
        already an advantage, but it can introduce bias in a network by always
        choosing the same type of experiences for training. In order to fight this, we
        compute the weight of the experience (this is called Importance Sampling,
        or IP). We want the weights to decrease over time, this is controlled by
        the beta parameter.
        &#34;&#34;&#34;
        # calculate probabilities (alpha)
        probabilities = self.priorities() ** self.alpha
        p = probabilities / probabilities.sum()
        # sample experiences
        buffer_size = len(self.experience_queue)
        samples = numpy.random.choice(a=buffer_size, size=self.batch_size, p=p, replace=False)
        experiences = [self.experience_queue[i].experience for i in samples]
        # importance Sampling
        # w_i = (1/N * 1/P_i) ^ beta
        weights = ((1 / buffer_size) * (1 / p[samples])) ** self.beta
        weights = weights / weights.max()
        self.update_beta()
        # return experiences with weights
        return list(zip(*experiences, strict=False)) + [tuple(weights)] + [tuple(samples)]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer" href="experience_buffer.html#deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer">ExperienceReplayBuffer</a></li>
<li><a title="deeprecsys.rl.experience_replay.experience_buffer.ExperienceBuffer" href="experience_buffer.html#deeprecsys.rl.experience_replay.experience_buffer.ExperienceBuffer">ExperienceBuffer</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.priorities"><code class="name flex">
<span>def <span class="ident">priorities</span></span>(<span>self) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Get the priority of each experience in the queue</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def priorities(self) -&gt; numpy.array:
    &#34;&#34;&#34;Get the priority of each experience in the queue&#34;&#34;&#34;
    return numpy.array([e.priority for e in self.experience_queue], dtype=numpy.float32)</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.sample_batch"><code class="name flex">
<span>def <span class="ident">sample_batch</span></span>(<span>self) ‑> List[Tuple]</span>
</code></dt>
<dd>
<div class="desc"><p>We sample experiences using their priorities as weights for sampling. The
effect of the priorities is controlled by the alpha parameter. This is
already an advantage, but it can introduce bias in a network by always
choosing the same type of experiences for training. In order to fight this, we
compute the weight of the experience (this is called Importance Sampling,
or IP). We want the weights to decrease over time, this is controlled by
the beta parameter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_batch(self) -&gt; List[Tuple]:
    &#34;&#34;&#34;We sample experiences using their priorities as weights for sampling. The
    effect of the priorities is controlled by the alpha parameter. This is
    already an advantage, but it can introduce bias in a network by always
    choosing the same type of experiences for training. In order to fight this, we
    compute the weight of the experience (this is called Importance Sampling,
    or IP). We want the weights to decrease over time, this is controlled by
    the beta parameter.
    &#34;&#34;&#34;
    # calculate probabilities (alpha)
    probabilities = self.priorities() ** self.alpha
    p = probabilities / probabilities.sum()
    # sample experiences
    buffer_size = len(self.experience_queue)
    samples = numpy.random.choice(a=buffer_size, size=self.batch_size, p=p, replace=False)
    experiences = [self.experience_queue[i].experience for i in samples]
    # importance Sampling
    # w_i = (1/N * 1/P_i) ^ beta
    weights = ((1 / buffer_size) * (1 / p[samples])) ** self.beta
    weights = weights / weights.max()
    self.update_beta()
    # return experiences with weights
    return list(zip(*experiences, strict=False)) + [tuple(weights)] + [tuple(samples)]</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.store_experience"><code class="name flex">
<span>def <span class="ident">store_experience</span></span>(<span>self, state: Any, action: Any, reward: float, done: bool, next_state: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>We include a priority to the experience. if the queue is empty, priority is 1 (max),
otherwise we check the maximum priority in the queue</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_experience(self, state: Any, action: Any, reward: float, done: bool, next_state: Any) -&gt; None:
    &#34;&#34;&#34;We include a priority to the experience. if the queue is empty, priority is 1 (max),
    otherwise we check the maximum priority in the queue
    &#34;&#34;&#34;
    priorities = self.priorities()
    priority = priorities.max() if len(priorities) &gt; 0 else 1.0
    if not np.isnan(priority):
        experience = Experience(state, action, reward, done, next_state)
        priority_experience = PriorityExperience(experience, priority)  # type: ignore
        # append to the right (end) of the queue
        self.experience_queue.append(priority_experience)</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_beta"><code class="name flex">
<span>def <span class="ident">update_beta</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>We want to grow the beta value slowly and linearly, starting at a value
close to zero, and stopping at 1.0. This is for the Importance Sampling</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_beta(self) -&gt; None:
    &#34;&#34;&#34;We want to grow the beta value slowly and linearly, starting at a value
    close to zero, and stopping at 1.0. This is for the Importance Sampling
    &#34;&#34;&#34;
    if self.beta &lt; 1.0:
        self.beta += self.beta_growth</code></pre>
</details>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_priorities"><code class="name flex">
<span>def <span class="ident">update_priorities</span></span>(<span>self, batch: List[Tuple], errors_from_batch: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>We want the priority of elements to be the TD error of plus an epsilon
constant. The epsilon constant makes sure that no experience ever gets a
priority zero. This prioritization strategy gives more importance to
elements that bring more learning to the network.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_priorities(self, batch: List[Tuple], errors_from_batch: List[float]) -&gt; None:
    &#34;&#34;&#34;We want the priority of elements to be the TD error of plus an epsilon
    constant. The epsilon constant makes sure that no experience ever gets a
    priority zero. This prioritization strategy gives more importance to
    elements that bring more learning to the network.
    &#34;&#34;&#34;
    experience_indexes = [b[-1] for b in numpy.array(batch, dtype=&#34;object&#34;).T]
    for i in range(len(experience_indexes)):
        error = abs(errors_from_batch[i]) + self.epsilon
        if not np.isnan(error):
            experience = self.experience_queue[experience_indexes[i]]
            experience._replace(priority=error)
            self.experience_queue[experience_indexes[i]] = experience</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer" href="experience_buffer.html#deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer">ExperienceReplayBuffer</a></b></code>:
<ul class="hlist">
<li><code><a title="deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer.ready_to_predict" href="experience_buffer.html#deeprecsys.rl.experience_replay.experience_buffer.ExperienceReplayBuffer.ready_to_predict">ready_to_predict</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience"><code class="flex name class">
<span>class <span class="ident">PriorityExperience</span></span>
<span>(</span><span>experience, priority)</span>
</code></dt>
<dd>
<div class="desc"><p>PriorityExperience(experience, priority)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.experience"><code class="name">var <span class="ident">experience</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.priority"><code class="name">var <span class="ident">priority</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deeprecsys.rl.experience_replay" href="index.html">deeprecsys.rl.experience_replay</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer">PrioritizedExperienceReplayBuffer</a></code></h4>
<ul class="">
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.priorities" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.priorities">priorities</a></code></li>
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.sample_batch" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.sample_batch">sample_batch</a></code></li>
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.store_experience" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.store_experience">store_experience</a></code></li>
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_beta" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_beta">update_beta</a></code></li>
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_priorities" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PrioritizedExperienceReplayBuffer.update_priorities">update_priorities</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience">PriorityExperience</a></code></h4>
<ul class="">
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.experience" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.experience">experience</a></code></li>
<li><code><a title="deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.priority" href="#deeprecsys.rl.experience_replay.priority_replay_buffer.PriorityExperience.priority">priority</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>