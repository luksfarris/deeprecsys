{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeeprecsys.rl.manager import MovieLensFairnessManager\n",
    "from pydeeprecsys.rl.agents.rainbow import RainbowDQNAgent\n",
    "from pydeeprecsys.rl.learning_statistics import LearningStatistics\n",
    "from pydeeprecsys.rl.agents.reinforce import ReinforceAgent\n",
    "from pydeeprecsys.rl.agents.agent import RandomAgent\n",
    "import pandas as pd\n",
    "\n",
    "training_iterations = 3\n",
    "training_episodes = 2000\n",
    "manager = MovieLensFairnessManager(slate_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_training(agent_class, params, trainings, episodes):\n",
    "    statistics = []\n",
    "    for i in range(trainings):\n",
    "        manager = MovieLensFairnessManager(slate_size=1)\n",
    "        stats = LearningStatistics()\n",
    "        agent = agent_class(**params)        \n",
    "        manager.train(agent, max_episodes=episodes, statistics=stats)\n",
    "        statistics.append(stats)\n",
    "    metrics = pd.DataFrame()\n",
    "    for i in range(len(statistics)):\n",
    "        stats = pd.DataFrame(statistics[i].collected_metrics)\n",
    "        stats['model'] = agent_class.__name__\n",
    "        metrics = pd.concat([metrics, stats])\n",
    "    metrics.to_csv(f'output/{agent_class.__name__}_optimized_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode 1999 Mean Rewards 33.03 Last Reward 41.22\t\tTraining...\n",
      "Episode 1999 Mean Rewards 34.60 Last Reward 48.30\t\tTraining...\n",
      "Episode 1999 Mean Rewards 33.41 Last Reward 45.71\t\tTraining...\n",
      "Episode 1999 Mean Rewards 38.50 Last Reward 47.29\t\tTraining...\n",
      "Episode 1999 Mean Rewards 34.40 Last Reward 38.10\t\t"
     ]
    }
   ],
   "source": [
    "reinforce_params = {\n",
    "    \"n_actions\": manager.env.action_space.n,\n",
    "    \"state_size\": manager.env.observation_space.shape[0],\n",
    "    \"hidden_layers\": [128, 128],\n",
    "    \"discount_factor\": 0.9,\n",
    "    \"learning_rate\": 0.001,\n",
    "}\n",
    "run_full_training(ReinforceAgent, reinforce_params, training_iterations, training_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode 1999 Mean Rewards 31.43 Last Reward 49.74\t\tTraining...\n",
      "Episode 1999 Mean Rewards 26.90 Last Reward 11.60\t\tTraining...\n",
      "Episode 1999 Mean Rewards 25.30 Last Reward 27.90\t\tTraining...\n",
      "Episode 1999 Mean Rewards 22.09 Last Reward 49.78\t\tTraining...\n",
      "Episode 1999 Mean Rewards 24.22 Last Reward 10.00\t\t"
     ]
    }
   ],
   "source": [
    "from pydeeprecsys.rl.agents.actor_critic import ActorCriticAgent\n",
    "ac_params = default_params = {\n",
    "    \"n_actions\": manager.env.action_space.n,\n",
    "    \"state_size\": manager.env.observation_space.shape[0],\n",
    "    \"actor_hidden_layers\": [128, 128],\n",
    "    \"critic_hidden_layers\": [128, 64],\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"actor_learning_rate\": 0.001,\n",
    "    \"critic_learning_rate\": 0.0001,\n",
    "}\n",
    "run_full_training(ActorCriticAgent, ac_params, training_iterations, training_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode 1999 Mean Rewards 40.77 Last Reward 50.00\t\tTraining...\n",
      "Episode 1999 Mean Rewards 36.29 Last Reward 42.23\t\tTraining...\n",
      "Episode 1999 Mean Rewards 36.42 Last Reward 48.80\t\t"
     ]
    }
   ],
   "source": [
    "from pydeeprecsys.rl.agents.rainbow import RainbowDQNAgent\n",
    "dqn_params = {\n",
    "    \"output_size\": manager.env.action_space.n,\n",
    "    \"input_size\": manager.env.observation_space.shape[0],\n",
    "    \"network_update_frequency\": 3,\n",
    "    \"network_sync_frequency\": 300,\n",
    "    \"priority_importance\": 0.4,\n",
    "    \"priority_weigth_growth\": 0.01,\n",
    "    \"buffer_size\": 10000,\n",
    "    \"buffer_burn_in\": 1000,\n",
    "    \"batch_size\": 32,\n",
    "    \"noise_sigma\": 0.017,\n",
    "    \"discount_factor\": 0.95,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"hidden_layers\": [512, 512, 128, 128],\n",
    "}\n",
    "run_full_training(RainbowDQNAgent, dqn_params, training_iterations, training_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('venv')",
   "language": "python",
   "name": "python3613jvsc74a57bd0b1eaf65962b2c757aebf41083ae472141ef09901ff07595586c177957debc092"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "b1eaf65962b2c757aebf41083ae472141ef09901ff07595586c177957debc092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
